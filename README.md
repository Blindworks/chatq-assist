# ChatQ Assist - AI-Powered FAQ Chatbot

Ein intelligenter, LLM-basierter FAQ-/Support-Chatbot fÃ¼r KMU mit **RAG (Retrieval-Augmented Generation)**, **Streaming-Antworten** und **intelligenter Performance-Optimierung**. Das System nutzt OpenAI GPT-4 und Vector-Similarity-Search, um prÃ¤zise Antworten auf Basis Ihrer FAQ-Datenbank zu generieren.

## ğŸš€ Features

### âœ… Implementiert (v0.1)

#### Core FunktionalitÃ¤t
- **ğŸ¤– LLM-basierter Chat**: OpenAI GPT-4 fÃ¼r natÃ¼rliche, kontextbewusste Antworten
- **ğŸ” RAG Pipeline**: Retrieval-Augmented Generation mit pgvector Similarity Search
- **âš¡ Streaming Responses**: Echtzeit-Antworten via Server-Sent Events (SSE)
- **ğŸ“ FAQ Management**: VollstÃ¤ndiges CRUD-System fÃ¼r FAQ-EintrÃ¤ge
- **ğŸ·ï¸ Tag-System**: Organisierung von FAQs mit Tags
- **ğŸ’¬ Conversation History**: Kontextbewusste Multi-Turn-GesprÃ¤che
- **ğŸ¯ Confidence Scoring**: Automatische QualitÃ¤tsbewertung der Antworten
- **ğŸ”„ Smart Handoff**: Automatische Weiterleitung bei fehlenden Antworten

#### Performance-Optimierung
- **ğŸ’¾ Embedding Cache**: Wiederholte Texte generieren keine neuen OpenAI-API-Aufrufe
- **ğŸš€ FAQ Match Cache**: Vector-Similarity-Suchergebnisse werden gecacht
- **ğŸ“¦ Batch Processing**: Bulk-Import von FAQs in einer Transaktion
- **â±ï¸ Cache TTL**: 24h TTL, 10.000 EintrÃ¤ge pro Cache (Caffeine)

#### Frontend
- **ğŸ¨ Modernes Chat-Widget**: Angular 17 Standalone Component
- **ğŸ“± Responsive Design**: Mobile-first UI mit Custom CSS
- **ğŸŒŠ Streaming UI**: Token-basierte Echtzeit-Anzeige
- **ğŸ’­ Source References**: Anzeige der verwendeten FAQ-Quellen

#### Multi-Tenancy
- **ğŸ¢ Tenant-Isolation**: Header-basierte Mandantentrennung (`X-Tenant-ID`)
- **ğŸ—„ï¸ Daten-Isolation**: Alle Queries filtern nach Tenant-ID

### ğŸš§ Geplant (Roadmap)

- [ ] **Document Ingestion**: URLs, PDFs, DOCX hochladen und verarbeiten
- [ ] **Analytics Dashboard**: Top-Fragen, Deflection-Rate, Confidence-Trends
- [ ] **Admin Panel**: Web-UI fÃ¼r FAQ-Management
- [ ] **E-Mail Notifications**: Bei Handoff automatisch benachrichtigen
- [ ] **Multi-Language Support**: i18n fÃ¼r verschiedene Sprachen
- [ ] **Rate Limiting**: API-Schutz vor Ãœberlastung
- [ ] **Monitoring & Metrics**: Prometheus/Grafana Integration

## ğŸ› ï¸ Tech Stack

### Backend
| Technologie | Version | Verwendung |
|------------|---------|-----------|
| **Spring Boot** | 3.2.0 | Application Framework |
| **Java** | 21 | Programming Language |
| **PostgreSQL** | 16+ | Primary Database |
| **pgvector** | latest | Vector Similarity Search |
| **LangChain4j** | 0.35.0 | LLM Integration Framework |
| **OpenAI API** | GPT-4 | Language Model |
| **OpenAI Embeddings** | text-embedding-3-small | Embedding Model (1536 dim) |
| **Flyway** | 9.x | Database Migration |
| **Caffeine** | latest | In-Memory Cache |
| **Lombok** | latest | Boilerplate Reduction |

### Frontend
| Technologie | Version | Verwendung |
|------------|---------|-----------|
| **Angular** | 17.x | Frontend Framework |
| **TypeScript** | 5.x | Programming Language |
| **RxJS** | 7.x | Reactive Programming |
| **HttpClient** | Angular | HTTP Communication |

### DevOps
- **Docker** + **Docker Compose**: Containerization
- **Maven**: Build Tool (Backend)
- **Angular CLI**: Build Tool (Frontend)

## ğŸ“ Projektstruktur

```
ChatQ-Assist/
â”œâ”€â”€ chatq-assist-backend/              # Spring Boot Backend
â”‚   â”œâ”€â”€ src/main/java/com/chatq/assist/
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â”œâ”€â”€ OpenAiConfig.java      # LangChain4j & OpenAI Configuration
â”‚   â”‚   â”‚   â””â”€â”€ CacheConfig.java       # Caffeine Cache Setup
â”‚   â”‚   â”œâ”€â”€ controller/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatController.java    # /api/chat (+ /stream)
â”‚   â”‚   â”‚   â””â”€â”€ FaqController.java     # /api/faq (+ /batch)
â”‚   â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”‚   â”œâ”€â”€ entity/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FaqEntry.java      # FAQ Entity mit Embeddings
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Conversation.java  # Chat Sessions
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Message.java       # Chat Messages
â”‚   â”‚   â”‚   â”œâ”€â”€ dto/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatRequest.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatResponse.java
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ FaqEntryDto.java
â”‚   â”‚   â”‚   â””â”€â”€ enums/
â”‚   â”‚   â”‚       â”œâ”€â”€ ConversationStatus.java
â”‚   â”‚   â”‚       â””â”€â”€ MessageRole.java
â”‚   â”‚   â”œâ”€â”€ repository/
â”‚   â”‚   â”‚   â”œâ”€â”€ FaqRepository.java     # inkl. Vector Similarity Query
â”‚   â”‚   â”‚   â”œâ”€â”€ ConversationRepository.java
â”‚   â”‚   â”‚   â””â”€â”€ MessageRepository.java
â”‚   â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatServiceLLM.java    # RAG Pipeline + Streaming
â”‚   â”‚   â”‚   â”œâ”€â”€ EmbeddingService.java  # OpenAI Embedding Generation
â”‚   â”‚   â”‚   â””â”€â”€ FaqService.java        # FAQ CRUD + Batch Import
â”‚   â”‚   â””â”€â”€ util/
â”‚   â”‚       â””â”€â”€ VectorType.java        # Hibernate pgvector UserType
â”‚   â”œâ”€â”€ src/main/resources/
â”‚   â”‚   â”œâ”€â”€ db/migration/
â”‚   â”‚   â”‚   â”œâ”€â”€ V1__init_schema.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ V2__add_chat_tables.sql
â”‚   â”‚   â”‚   â””â”€â”€ V3__add_embeddings_to_faq.sql
â”‚   â”‚   â””â”€â”€ application.properties
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ pom.xml
â”‚
â”œâ”€â”€ chatq-assist-frontend/             # Angular Widget
â”‚   â”œâ”€â”€ src/app/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â””â”€â”€ chat-widget/
â”‚   â”‚   â”‚       â”œâ”€â”€ chat-widget.component.ts    # Main Chat UI
â”‚   â”‚   â”‚       â”œâ”€â”€ chat-widget.component.html
â”‚   â”‚   â”‚       â””â”€â”€ chat-widget.component.css
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â””â”€â”€ chat.service.ts        # HTTP + SSE Handling
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ angular.json
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ CLAUDE.md                          # Project Instructions fÃ¼r Claude Code
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Voraussetzungen

- **Docker** & **Docker Compose** (fÃ¼r einfachsten Start)
- **Java 21** (fÃ¼r lokale Backend-Entwicklung)
- **Node.js 18+** (fÃ¼r Frontend-Entwicklung)
- **PostgreSQL 16+** mit **pgvector Extension** (wenn lokal ohne Docker)
- **OpenAI API Key** ([hier erstellen](https://platform.openai.com/api-keys))

### Option 1: Docker Compose (Empfohlen)

```bash
# 1. Repository klonen
git clone <repository-url>
cd ChatQ-Assist

# 2. Umgebungsvariablen setzen
# Erstelle docker-compose.yml oder setze Env-Vars:
export OPENAI_API_KEY="sk-..."

# 3. Starten
docker-compose up -d

# 4. Logs verfolgen
docker-compose logs -f backend
```

**Zugriff**:
- Backend API: http://localhost:8080
- Frontend Widget: http://localhost:4200
- PostgreSQL: localhost:5433 (User: `postgres`, PW: `taxcRH51#`)

### Option 2: Lokale Entwicklung

#### Backend starten

```bash
cd chatq-assist-backend

# Windows
mvnw.cmd spring-boot:run

# Linux/Mac
./mvnw spring-boot:run
```

**Wichtig**: PostgreSQL mit pgvector muss laufen! Siehe [pgvector Installation](#pgvector-installation).

#### Frontend starten

```bash
cd chatq-assist-frontend
npm install
ng serve
```

Widget lÃ¤uft auf http://localhost:4200

## ğŸ“‹ API Dokumentation

### Chat Endpoints

#### POST /api/chat
Standard-Chat (nicht-streaming)

**Request**:
```json
{
  "question": "Was sind Ihre Ã–ffnungszeiten?",
  "sessionId": "optional-uuid",
  "userEmail": "user@example.com"
}
```

**Response**:
```json
{
  "sessionId": "uuid",
  "answer": "Unsere Ã–ffnungszeiten sind...",
  "confidenceScore": 0.85,
  "sources": [
    {
      "type": "FAQ",
      "title": "Ã–ffnungszeiten",
      "id": 42
    }
  ],
  "handoffTriggered": false
}
```

#### POST /api/chat/stream
Chat mit Streaming-Antwort (SSE)

**Request**: Gleich wie `/api/chat`

**Response**: Server-Sent Events
```
event: token
data: Unsere

event: token
data:  Ã–ffnungszeiten

event: metadata
data: {"sessionId":"uuid","confidenceScore":0.85,"sources":[...],"handoffTriggered":false}
```

### FAQ Management Endpoints

#### GET /api/faq
Alle FAQs fÃ¼r einen Tenant abrufen

**Headers**: `X-Tenant-ID: default-tenant`

**Response**:
```json
[
  {
    "id": 1,
    "question": "Was sind Ihre Ã–ffnungszeiten?",
    "answer": "Montag bis Freitag 9-18 Uhr",
    "tags": ["zeiten", "service"],
    "isActive": true,
    "displayOrder": 1,
    "usageCount": 42,
    "createdAt": "2024-01-15T10:00:00Z",
    "updatedAt": "2024-01-15T10:00:00Z"
  }
]
```

#### POST /api/faq
Einzelnen FAQ-Eintrag erstellen

**Headers**: `X-Tenant-ID: default-tenant`

**Request**:
```json
{
  "question": "Wie kann ich Sie erreichen?",
  "answer": "Sie erreichen uns telefonisch unter...",
  "tags": ["kontakt"],
  "isActive": true,
  "displayOrder": 2
}
```

**Response**: Erstellter FAQ-Eintrag mit ID

#### POST /api/faq/batch
Mehrere FAQs auf einmal erstellen (Performance-optimiert)

**Headers**: `X-Tenant-ID: default-tenant`

**Request**:
```json
[
  {
    "question": "Frage 1?",
    "answer": "Antwort 1",
    "tags": ["tag1"],
    "displayOrder": 1
  },
  {
    "question": "Frage 2?",
    "answer": "Antwort 2",
    "tags": ["tag2"],
    "displayOrder": 2
  }
]
```

**Vorteile**:
- âœ… Einzelne Datenbank-Transaktion
- âœ… Embedding-Cache wird genutzt
- âœ… ~70% schneller als einzelne POST-Requests

#### PUT /api/faq/{id}
FAQ-Eintrag aktualisieren

**Headers**: `X-Tenant-ID: default-tenant`

**Request**: Gleich wie POST, aber auf bestehender ID

**Hinweis**: Embedding wird automatisch neu generiert!

#### DELETE /api/faq/{id}
FAQ-Eintrag lÃ¶schen

## ğŸ”§ Konfiguration

### application.properties

```properties
# Database
spring.datasource.url=jdbc:postgresql://localhost:5433/chatq_assist
spring.datasource.username=postgres
spring.datasource.password=taxcRH51#

# OpenAI
openai.api.key=${OPENAI_API_KEY}
openai.model.chat=gpt-4
openai.model.embedding=text-embedding-3-small

# Server
server.port=8080

# JPA (Flyway Ã¼bernimmt Schema)
spring.jpa.hibernate.ddl-auto=validate
```

### Environment Variables

| Variable | Beschreibung | Default |
|----------|--------------|---------|
| `OPENAI_API_KEY` | OpenAI API Key | *required* |
| `DB_HOST` | PostgreSQL Host | `localhost` |
| `DB_PORT` | PostgreSQL Port | `5433` |
| `DB_NAME` | Database Name | `chatq_assist` |
| `DB_USER` | Database User | `postgres` |
| `DB_PASSWORD` | Database Password | `taxcRH51#` |

## ğŸ¯ RAG Pipeline Explained

```
User Question: "Was sind Ihre Ã–ffnungszeiten?"
        â†“
[1] Embedding Generation (EmbeddingService)
    â†’ OpenAI text-embedding-3-small
    â†’ float[1536] vector
    â†’ Cached in Caffeine (24h TTL)
        â†“
[2] Vector Similarity Search (FaqRepository)
    â†’ pgvector <=> operator (cosine distance)
    â†’ SELECT * FROM faq_entries
      ORDER BY embedding <=> query_embedding
      LIMIT 3
    â†’ Results cached in Caffeine
        â†“
[3] Context Building (ChatServiceLLM)
    â†’ Top 3 FAQs als Context
    â†’ Conversation History (last 5 messages)
        â†“
[4] LLM Generation (OpenAI GPT-4)
    â†’ System Prompt + Context + History + Question
    â†’ Streaming via SSE
        â†“
[5] Response Processing
    â†’ Save to Message table
    â†’ Increment FAQ usage_count
    â†’ Return to user
```

## ğŸ§ª Testing

### IntelliJ HTTP Client

Erstelle eine `test.http` Datei:

```http
### Create FAQ
POST http://localhost:8080/api/faq
Content-Type: application/json
X-Tenant-ID: default-tenant

{
  "question": "Was sind Ihre Ã–ffnungszeiten?",
  "answer": "Montag bis Freitag von 9 bis 18 Uhr.",
  "tags": ["zeiten", "service"],
  "displayOrder": 1
}

### Get all FAQs
GET http://localhost:8080/api/faq
X-Tenant-ID: default-tenant

### Chat Request
POST http://localhost:8080/api/chat
Content-Type: application/json
X-Tenant-ID: default-tenant

{
  "question": "Wann habt ihr geÃ¶ffnet?",
  "sessionId": "test-session-123"
}

### Streaming Chat
POST http://localhost:8080/api/chat/stream
Content-Type: application/json
X-Tenant-ID: default-tenant

{
  "question": "Wie erreiche ich euch?",
  "sessionId": "test-session-456"
}
```

### cURL Examples

```bash
# Create FAQ
curl -X POST http://localhost:8080/api/faq \
  -H "Content-Type: application/json" \
  -H "X-Tenant-ID: default-tenant" \
  -d '{
    "question": "Was kostet der Service?",
    "answer": "Unsere Preise beginnen bei 99â‚¬/Monat.",
    "tags": ["preise"]
  }'

# Chat
curl -X POST http://localhost:8080/api/chat \
  -H "Content-Type: application/json" \
  -H "X-Tenant-ID: default-tenant" \
  -d '{
    "question": "Was kostet das?",
    "sessionId": "curl-test"
  }'
```

## ğŸ—„ï¸ Database Schema

### faq_entries
```sql
CREATE TABLE faq_entries (
    id BIGSERIAL PRIMARY KEY,
    tenant_id VARCHAR(255) NOT NULL,
    question TEXT NOT NULL,
    answer TEXT NOT NULL,
    tags TEXT[],
    is_active BOOLEAN DEFAULT true,
    display_order INTEGER,
    usage_count BIGINT DEFAULT 0,
    embedding vector(1536),  -- pgvector!
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    version INTEGER DEFAULT 0
);

CREATE INDEX idx_faq_tenant ON faq_entries(tenant_id);
CREATE INDEX idx_faq_embedding ON faq_entries
    USING ivfflat (embedding vector_cosine_ops);
```

### conversations
```sql
CREATE TABLE conversations (
    id BIGSERIAL PRIMARY KEY,
    session_id VARCHAR(255) UNIQUE NOT NULL,
    tenant_id VARCHAR(255) NOT NULL,
    user_email VARCHAR(255),
    status VARCHAR(50) DEFAULT 'ACTIVE',
    last_activity_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### messages
```sql
CREATE TABLE messages (
    id BIGSERIAL PRIMARY KEY,
    conversation_id BIGINT REFERENCES conversations(id),
    role VARCHAR(50) NOT NULL,  -- USER, ASSISTANT
    content TEXT NOT NULL,
    confidence_score DOUBLE PRECISION,
    faq_entry_id BIGINT,
    tenant_id VARCHAR(255) NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);
```

## ğŸ” pgvector Installation

### Windows (PostgreSQL 16+)

1. Download pgvector fÃ¼r deine PostgreSQL-Version von:
   https://github.com/pgvector/pgvector/releases

2. Extrahiere `vector.dll` nach:
   ```
   C:\Program Files\PostgreSQL\16\lib\
   ```

3. Extrahiere SQL-Dateien nach:
   ```
   C:\Program Files\PostgreSQL\16\share\extension\
   ```

4. In psql:
   ```sql
   CREATE EXTENSION vector;
   ```

### Linux (Ubuntu/Debian)

```bash
sudo apt install postgresql-16-pgvector
```

### macOS

```bash
brew install pgvector
```

### Docker

```dockerfile
FROM postgres:16
RUN apt-get update && apt-get install -y postgresql-16-pgvector
```

## ğŸ“Š Performance Benchmarks

### Ohne Caching
- Erste Anfrage: ~2.5s (Embedding: 200ms + Vector Search: 100ms + GPT-4: 2.2s)
- Wiederholte Anfrage: ~2.5s (kein Cache)

### Mit Caching (aktuell)
- Erste Anfrage: ~2.5s
- Wiederholte Anfrage: ~2.2s (Embedding cached: -200ms)
- Identische FAQ-Suche: ~2.0s (Embedding + Search cached: -300ms)

### Batch Import
- 100 FAQs einzeln: ~45s
- 100 FAQs batch: ~15s (**~70% schneller**)

## ğŸš¨ Troubleshooting

### "Typ vector existiert nicht"
â†’ pgvector Extension nicht installiert. Siehe [pgvector Installation](#pgvector-installation)

### "Die Abfrage lieferte kein Ergebnis" (Hibernate)
â†’ PostgreSQL dependency muss `compile` scope haben (nicht `runtime`)

### Tokens ohne Leerzeichen im Frontend
â†’ SSE Parser darf `data:` nicht `.trim()`en! Siehe `chat.service.ts:97`

### OpenAI API Rate Limit
â†’ Upgrade auf Tier 2+, oder verwende lÃ¤ngeres Rate Limiting

### pgvector Index langsam
â†’ Bei >10.000 FAQs: `CREATE INDEX USING ivfflat ... WITH (lists = 100)`

## ğŸ“ˆ Monitoring

### Cache Statistics

```java
// In CacheConfig ist recordStats() aktiviert
CacheManager cacheManager = ...;
Cache cache = cacheManager.getCache("embeddings");
CaffeineCache caffeineCache = (CaffeineCache) cache;
com.github.benmanes.caffeine.cache.Cache nativeCache =
    caffeineCache.getNativeCache();
CacheStats stats = nativeCache.stats();

// Hit Rate, Miss Rate, Evictions, etc.
```

### Spring Boot Actuator

Endpoints verfÃ¼gbar (wenn aktiviert):
- `/actuator/health` - Health Check
- `/actuator/metrics` - Metriken
- `/actuator/caches` - Cache-Informationen

## ğŸ¤ Contributing

1. Fork das Repository
2. Feature-Branch erstellen: `git checkout -b feature/amazing-feature`
3. Committen: `git commit -m 'Add amazing feature'`
4. Push: `git push origin feature/amazing-feature`
5. Pull Request Ã¶ffnen

## ğŸ“ Lizenz

Dieses Projekt ist unter der **MIT License** lizenziert - siehe [LICENSE](LICENSE) fÃ¼r Details.

## ğŸ™‹ Support

- **Issues**: [GitHub Issues](https://github.com/your-org/chatq-assist/issues)
- **Email**: support@your-domain.com
- **Docs**: [Wiki](https://github.com/your-org/chatq-assist/wiki)

## ğŸ‘ Credits

- **LangChain4j**: https://github.com/langchain4j/langchain4j
- **pgvector**: https://github.com/pgvector/pgvector
- **OpenAI**: https://openai.com

---

**Built with â¤ï¸ for SMBs who need smart, privacy-focused customer support**
