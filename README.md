# ChatQ Assist - AI-Powered FAQ Chatbot

An intelligent, LLM-based FAQ/Support chatbot for SMBs featuring **RAG (Retrieval-Augmented Generation)**, **streaming responses**, and **intelligent performance optimization**. The system uses OpenAI GPT-4 and vector similarity search to generate precise answers based on your FAQ database.

## ğŸš€ Features

### âœ… Implemented (v0.1)

#### Core Functionality
- **ğŸ¤– LLM-based Chat**: OpenAI GPT-4 for natural, context-aware responses
- **ğŸ” RAG Pipeline**: Retrieval-Augmented Generation with pgvector similarity search
- **âš¡ Streaming Responses**: Real-time answers via Server-Sent Events (SSE)
- **ğŸ“ FAQ Management**: Complete CRUD system for FAQ entries
- **ğŸ·ï¸ Tag System**: Organize FAQs with tags
- **ğŸ’¬ Conversation History**: Context-aware multi-turn conversations
- **ğŸ¯ Confidence Scoring**: Automatic quality assessment of responses
- **ğŸ”„ Smart Handoff**: Automatic escalation when no answer is found

#### Performance Optimization
- **ğŸ’¾ Embedding Cache**: Repeated texts don't generate new OpenAI API calls
- **ğŸš€ FAQ Match Cache**: Vector similarity search results are cached
- **ğŸ“¦ Batch Processing**: Bulk import of FAQs in a single transaction
- **â±ï¸ Cache TTL**: 24h TTL, 10,000 entries per cache (Caffeine)

#### Frontend
- **ğŸ¨ Modern Chat Widget**: Angular 17 Standalone Component
- **ğŸ“± Responsive Design**: Mobile-first UI with custom CSS
- **ğŸŒŠ Streaming UI**: Token-based real-time display
- **ğŸ’­ Source References**: Display of used FAQ sources

#### Multi-Tenancy
- **ğŸ¢ Tenant Isolation**: Header-based tenant separation (`X-Tenant-ID`)
- **ğŸ—„ï¸ Data Isolation**: All queries filter by tenant ID

### ğŸš§ Planned (Roadmap)

- [ ] **Document Ingestion**: Upload and process URLs, PDFs, DOCX
- [ ] **Analytics Dashboard**: Top questions, deflection rate, confidence trends
- [ ] **Admin Panel**: Web UI for FAQ management
- [ ] **E-Mail Notifications**: Automatic notifications on handoff
- [ ] **Multi-Language Support**: i18n for different languages
- [ ] **Rate Limiting**: API protection against overload
- [ ] **Monitoring & Metrics**: Prometheus/Grafana integration

## ğŸ› ï¸ Tech Stack

### Backend
| Technology | Version | Purpose |
|------------|---------|---------|
| **Spring Boot** | 3.2.0 | Application Framework |
| **Java** | 21 | Programming Language |
| **PostgreSQL** | 16+ | Primary Database |
| **pgvector** | latest | Vector Similarity Search |
| **LangChain4j** | 0.35.0 | LLM Integration Framework |
| **OpenAI API** | GPT-4 | Language Model |
| **OpenAI Embeddings** | text-embedding-3-small | Embedding Model (1536 dim) |
| **Flyway** | 9.x | Database Migration |
| **Caffeine** | latest | In-Memory Cache |
| **Lombok** | latest | Boilerplate Reduction |

### Frontend
| Technology | Version | Purpose |
|------------|---------|---------|
| **Angular** | 17.x | Frontend Framework |
| **TypeScript** | 5.x | Programming Language |
| **RxJS** | 7.x | Reactive Programming |
| **HttpClient** | Angular | HTTP Communication |

### DevOps
- **Docker** + **Docker Compose**: Containerization
- **Maven**: Build Tool (Backend)
- **Angular CLI**: Build Tool (Frontend)

## ğŸ“ Project Structure

```
ChatQ-Assist/
â”œâ”€â”€ chatq-assist-backend/              # Spring Boot Backend
â”‚   â”œâ”€â”€ src/main/java/com/chatq/assist/
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â”œâ”€â”€ OpenAiConfig.java      # LangChain4j & OpenAI Configuration
â”‚   â”‚   â”‚   â””â”€â”€ CacheConfig.java       # Caffeine Cache Setup
â”‚   â”‚   â”œâ”€â”€ controller/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatController.java    # /api/chat (+ /stream)
â”‚   â”‚   â”‚   â””â”€â”€ FaqController.java     # /api/faq (+ /batch)
â”‚   â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”‚   â”œâ”€â”€ entity/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FaqEntry.java      # FAQ Entity with Embeddings
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Conversation.java  # Chat Sessions
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Message.java       # Chat Messages
â”‚   â”‚   â”‚   â”œâ”€â”€ dto/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatRequest.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatResponse.java
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ FaqEntryDto.java
â”‚   â”‚   â”‚   â””â”€â”€ enums/
â”‚   â”‚   â”‚       â”œâ”€â”€ ConversationStatus.java
â”‚   â”‚   â”‚       â””â”€â”€ MessageRole.java
â”‚   â”‚   â”œâ”€â”€ repository/
â”‚   â”‚   â”‚   â”œâ”€â”€ FaqRepository.java     # incl. Vector Similarity Query
â”‚   â”‚   â”‚   â”œâ”€â”€ ConversationRepository.java
â”‚   â”‚   â”‚   â””â”€â”€ MessageRepository.java
â”‚   â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatServiceLLM.java    # RAG Pipeline + Streaming
â”‚   â”‚   â”‚   â”œâ”€â”€ EmbeddingService.java  # OpenAI Embedding Generation
â”‚   â”‚   â”‚   â””â”€â”€ FaqService.java        # FAQ CRUD + Batch Import
â”‚   â”‚   â””â”€â”€ util/
â”‚   â”‚       â””â”€â”€ VectorType.java        # Hibernate pgvector UserType
â”‚   â”œâ”€â”€ src/main/resources/
â”‚   â”‚   â”œâ”€â”€ db/migration/
â”‚   â”‚   â”‚   â”œâ”€â”€ V1__init_schema.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ V2__add_chat_tables.sql
â”‚   â”‚   â”‚   â””â”€â”€ V3__add_embeddings_to_faq.sql
â”‚   â”‚   â””â”€â”€ application.properties
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ pom.xml
â”‚
â”œâ”€â”€ chatq-assist-frontend/             # Angular Widget
â”‚   â”œâ”€â”€ src/app/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â””â”€â”€ chat-widget/
â”‚   â”‚   â”‚       â”œâ”€â”€ chat-widget.component.ts    # Main Chat UI
â”‚   â”‚   â”‚       â”œâ”€â”€ chat-widget.component.html
â”‚   â”‚   â”‚       â””â”€â”€ chat-widget.component.css
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â””â”€â”€ chat.service.ts        # HTTP + SSE Handling
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ angular.json
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ CLAUDE.md                          # Project Instructions for Claude Code
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites

- **Docker** & **Docker Compose** (for easiest setup)
- **Java 21** (for local backend development)
- **Node.js 18+** (for frontend development)
- **PostgreSQL 16+** with **pgvector extension** (if running locally without Docker)
- **OpenAI API Key** ([create one here](https://platform.openai.com/api-keys))

### Option 1: Docker Compose (Recommended)

```bash
# 1. Clone repository
git clone <repository-url>
cd ChatQ-Assist

# 2. Set environment variables
# Create docker-compose.yml or set env vars:
export OPENAI_API_KEY="sk-..."

# 3. Start
docker-compose up -d

# 4. Follow logs
docker-compose logs -f backend
```

**Access**:
- Backend API: http://localhost:8080
- Frontend Widget: http://localhost:4200
- PostgreSQL: localhost:5433 (User: `postgres`, PW: `taxcRH51#`)

### Option 2: Local Development

#### Start Backend

```bash
cd chatq-assist-backend

# Windows
mvnw.cmd spring-boot:run

# Linux/Mac
./mvnw spring-boot:run
```

**Important**: PostgreSQL with pgvector must be running! See [pgvector Installation](#pgvector-installation).

#### Start Frontend

```bash
cd chatq-assist-frontend
npm install
ng serve
```

Widget runs on http://localhost:4200

## ğŸ“‹ API Documentation

### Chat Endpoints

#### POST /api/chat
Standard chat (non-streaming)

**Request**:
```json
{
  "question": "What are your business hours?",
  "sessionId": "optional-uuid",
  "userEmail": "user@example.com"
}
```

**Response**:
```json
{
  "sessionId": "uuid",
  "answer": "Our business hours are...",
  "confidenceScore": 0.85,
  "sources": [
    {
      "type": "FAQ",
      "title": "Business Hours",
      "id": 42
    }
  ],
  "handoffTriggered": false
}
```

#### POST /api/chat/stream
Chat with streaming response (SSE)

**Request**: Same as `/api/chat`

**Response**: Server-Sent Events
```
event: token
data: Our

event: token
data:  business

event: metadata
data: {"sessionId":"uuid","confidenceScore":0.85,"sources":[...],"handoffTriggered":false}
```

### FAQ Management Endpoints

#### GET /api/faq
Get all FAQs for a tenant

**Headers**: `X-Tenant-ID: default-tenant`

**Response**:
```json
[
  {
    "id": 1,
    "question": "What are your business hours?",
    "answer": "Monday to Friday 9am-6pm",
    "tags": ["hours", "service"],
    "isActive": true,
    "displayOrder": 1,
    "usageCount": 42,
    "createdAt": "2024-01-15T10:00:00Z",
    "updatedAt": "2024-01-15T10:00:00Z"
  }
]
```

#### POST /api/faq
Create a single FAQ entry

**Headers**: `X-Tenant-ID: default-tenant`

**Request**:
```json
{
  "question": "How can I contact you?",
  "answer": "You can reach us by phone at...",
  "tags": ["contact"],
  "isActive": true,
  "displayOrder": 2
}
```

**Response**: Created FAQ entry with ID

#### POST /api/faq/batch
Create multiple FAQs at once (performance-optimized)

**Headers**: `X-Tenant-ID: default-tenant`

**Request**:
```json
[
  {
    "question": "Question 1?",
    "answer": "Answer 1",
    "tags": ["tag1"],
    "displayOrder": 1
  },
  {
    "question": "Question 2?",
    "answer": "Answer 2",
    "tags": ["tag2"],
    "displayOrder": 2
  }
]
```

**Benefits**:
- âœ… Single database transaction
- âœ… Embedding cache is utilized
- âœ… ~70% faster than individual POST requests

#### PUT /api/faq/{id}
Update FAQ entry

**Headers**: `X-Tenant-ID: default-tenant`

**Request**: Same as POST, but on existing ID

**Note**: Embedding is automatically regenerated!

#### DELETE /api/faq/{id}
Delete FAQ entry

## ğŸ”§ Configuration

### application.properties

```properties
# Database
spring.datasource.url=jdbc:postgresql://localhost:5433/chatq_assist
spring.datasource.username=postgres
spring.datasource.password=taxcRH51#

# OpenAI
openai.api.key=${OPENAI_API_KEY}
openai.model.chat=gpt-4
openai.model.embedding=text-embedding-3-small

# Server
server.port=8080

# JPA (Flyway handles schema)
spring.jpa.hibernate.ddl-auto=validate
```

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | OpenAI API Key | *required* |
| `DB_HOST` | PostgreSQL Host | `localhost` |
| `DB_PORT` | PostgreSQL Port | `5433` |
| `DB_NAME` | Database Name | `chatq_assist` |
| `DB_USER` | Database User | `postgres` |
| `DB_PASSWORD` | Database Password | `taxcRH51#` |

## ğŸ¯ RAG Pipeline Explained

```
User Question: "What are your business hours?"
        â†“
[1] Embedding Generation (EmbeddingService)
    â†’ OpenAI text-embedding-3-small
    â†’ float[1536] vector
    â†’ Cached in Caffeine (24h TTL)
        â†“
[2] Vector Similarity Search (FaqRepository)
    â†’ pgvector <=> operator (cosine distance)
    â†’ SELECT * FROM faq_entries
      ORDER BY embedding <=> query_embedding
      LIMIT 3
    â†’ Results cached in Caffeine
        â†“
[3] Context Building (ChatServiceLLM)
    â†’ Top 3 FAQs as context
    â†’ Conversation history (last 5 messages)
        â†“
[4] LLM Generation (OpenAI GPT-4)
    â†’ System prompt + Context + History + Question
    â†’ Streaming via SSE
        â†“
[5] Response Processing
    â†’ Save to Message table
    â†’ Increment FAQ usage_count
    â†’ Return to user
```

## ğŸ§ª Testing

### IntelliJ HTTP Client

Create a `test.http` file:

```http
### Create FAQ
POST http://localhost:8080/api/faq
Content-Type: application/json
X-Tenant-ID: default-tenant

{
  "question": "What are your business hours?",
  "answer": "Monday to Friday from 9am to 6pm.",
  "tags": ["hours", "service"],
  "displayOrder": 1
}

### Get all FAQs
GET http://localhost:8080/api/faq
X-Tenant-ID: default-tenant

### Chat Request
POST http://localhost:8080/api/chat
Content-Type: application/json
X-Tenant-ID: default-tenant

{
  "question": "When are you open?",
  "sessionId": "test-session-123"
}

### Streaming Chat
POST http://localhost:8080/api/chat/stream
Content-Type: application/json
X-Tenant-ID: default-tenant

{
  "question": "How can I reach you?",
  "sessionId": "test-session-456"
}
```

### cURL Examples

```bash
# Create FAQ
curl -X POST http://localhost:8080/api/faq \
  -H "Content-Type: application/json" \
  -H "X-Tenant-ID: default-tenant" \
  -d '{
    "question": "What does the service cost?",
    "answer": "Our prices start at $99/month.",
    "tags": ["pricing"]
  }'

# Chat
curl -X POST http://localhost:8080/api/chat \
  -H "Content-Type: application/json" \
  -H "X-Tenant-ID: default-tenant" \
  -d '{
    "question": "What does it cost?",
    "sessionId": "curl-test"
  }'
```

## ğŸ—„ï¸ Database Schema

### faq_entries
```sql
CREATE TABLE faq_entries (
    id BIGSERIAL PRIMARY KEY,
    tenant_id VARCHAR(255) NOT NULL,
    question TEXT NOT NULL,
    answer TEXT NOT NULL,
    tags TEXT[],
    is_active BOOLEAN DEFAULT true,
    display_order INTEGER,
    usage_count BIGINT DEFAULT 0,
    embedding vector(1536),  -- pgvector!
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    version INTEGER DEFAULT 0
);

CREATE INDEX idx_faq_tenant ON faq_entries(tenant_id);
CREATE INDEX idx_faq_embedding ON faq_entries
    USING ivfflat (embedding vector_cosine_ops);
```

### conversations
```sql
CREATE TABLE conversations (
    id BIGSERIAL PRIMARY KEY,
    session_id VARCHAR(255) UNIQUE NOT NULL,
    tenant_id VARCHAR(255) NOT NULL,
    user_email VARCHAR(255),
    status VARCHAR(50) DEFAULT 'ACTIVE',
    last_activity_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### messages
```sql
CREATE TABLE messages (
    id BIGSERIAL PRIMARY KEY,
    conversation_id BIGINT REFERENCES conversations(id),
    role VARCHAR(50) NOT NULL,  -- USER, ASSISTANT
    content TEXT NOT NULL,
    confidence_score DOUBLE PRECISION,
    faq_entry_id BIGINT,
    tenant_id VARCHAR(255) NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);
```

## ğŸ” pgvector Installation

### Windows (PostgreSQL 16+)

1. Download pgvector for your PostgreSQL version from:
   https://github.com/pgvector/pgvector/releases

2. Extract `vector.dll` to:
   ```
   C:\Program Files\PostgreSQL\16\lib\
   ```

3. Extract SQL files to:
   ```
   C:\Program Files\PostgreSQL\16\share\extension\
   ```

4. In psql:
   ```sql
   CREATE EXTENSION vector;
   ```

### Linux (Ubuntu/Debian)

```bash
sudo apt install postgresql-16-pgvector
```

### macOS

```bash
brew install pgvector
```

### Docker

```dockerfile
FROM postgres:16
RUN apt-get update && apt-get install -y postgresql-16-pgvector
```

## ğŸ“Š Performance Benchmarks

### Without Caching
- First request: ~2.5s (Embedding: 200ms + Vector Search: 100ms + GPT-4: 2.2s)
- Repeated request: ~2.5s (no cache)

### With Caching (current)
- First request: ~2.5s
- Repeated request: ~2.2s (Embedding cached: -200ms)
- Identical FAQ search: ~2.0s (Embedding + Search cached: -300ms)

### Batch Import
- 100 FAQs individually: ~45s
- 100 FAQs batch: ~15s (**~70% faster**)

## ğŸš¨ Troubleshooting

### "Type vector does not exist"
â†’ pgvector extension not installed. See [pgvector Installation](#pgvector-installation)

### "Query returned no result" (Hibernate)
â†’ PostgreSQL dependency must have `compile` scope (not `runtime`)

### Tokens without spaces in frontend
â†’ SSE parser must not `.trim()` the `data:` field! See `chat.service.ts:97`

### OpenAI API Rate Limit
â†’ Upgrade to Tier 2+, or implement longer rate limiting

### pgvector index slow
â†’ For >10,000 FAQs: `CREATE INDEX USING ivfflat ... WITH (lists = 100)`

## ğŸ“ˆ Monitoring

### Cache Statistics

```java
// recordStats() is enabled in CacheConfig
CacheManager cacheManager = ...;
Cache cache = cacheManager.getCache("embeddings");
CaffeineCache caffeineCache = (CaffeineCache) cache;
com.github.benmanes.caffeine.cache.Cache nativeCache =
    caffeineCache.getNativeCache();
CacheStats stats = nativeCache.stats();

// Hit Rate, Miss Rate, Evictions, etc.
```

### Spring Boot Actuator

Available endpoints (when enabled):
- `/actuator/health` - Health Check
- `/actuator/metrics` - Metrics
- `/actuator/caches` - Cache information

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push: `git push origin feature/amazing-feature`
5. Open Pull Request

## ğŸ“ License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) for details.

## ğŸ™‹ Support

- **Issues**: [GitHub Issues](https://github.com/your-org/chatq-assist/issues)
- **Email**: support@your-domain.com
- **Docs**: [Wiki](https://github.com/your-org/chatq-assist/wiki)

## ğŸ‘ Credits

- **LangChain4j**: https://github.com/langchain4j/langchain4j
- **pgvector**: https://github.com/pgvector/pgvector
- **OpenAI**: https://openai.com

---

**Built with â¤ï¸ for SMBs who need smart, privacy-focused customer support**
